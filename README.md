# Multi Context based-Knowledge-distilation

Among these the knowledge distillation, is a general purpose technique that at first glance is widely applicable and complements all other ways of compressing neural networks . The key idea is to use soft probabilities (or ‘logits’) of a larger “teacher network” to supervise a smaller “student” network, in addition to the available class labels. These soft probabilities reveal more information than the class labels alone, and can purportedly help the student network learn better.

**knowledge dist photo**

**proposed architecture**
images and content 

*dataset used*

*teacher model -Resnet about and images*

Steps to train teacher model 

Student training 
  *student 1 - images  and content  
  *student 2 - images and content 
Distilation loss function 

steps to train student using KD 

selector model - why , how para and model 

steps to train 

inferencing 


results 
